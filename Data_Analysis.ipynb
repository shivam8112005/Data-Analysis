{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from io import StringIO\n",
    "import os\n",
    "import textwrap\n",
    "from io import StringIO\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Genreal_Report(corr_matrix, dup, head, tail, des, null, info, shape):\n",
    "   \n",
    "     try:\n",
    "        folder_path = input(\"Enter the full path or name of the folder to save the report and heatmap: \").strip()\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        report = os.path.join(folder_path, \"report.txt\")\n",
    "        heatmap_file = os.path.join(folder_path, \"correlation_heatmap.png\")\n",
    "       \n",
    "\n",
    "        rows, cols = corr_matrix.shape\n",
    "        max_label_length = max(len(label) for label in corr_matrix.columns)\n",
    "\n",
    "        fig_width = max(8, cols * 0.5, max_label_length * 0.5)\n",
    "        fig_height = max(6, rows * 0.5)\n",
    "\n",
    "        plt.figure(figsize=(fig_width, fig_height))\n",
    "        plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.xticks(np.arange(cols), corr_matrix.columns, rotation=45, ha=\"right\", fontsize=12)\n",
    "        plt.yticks(np.arange(rows), corr_matrix.columns, rotation=0, fontsize=12)\n",
    "        plt.title('Correlation Matrix Heatmap', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(heatmap_file)  \n",
    "        plt.close()\n",
    "        with open(report, 'w') as f:\n",
    "            f.write(\"GENERAL ANALYSIS OF THE DATAFRAME\\n\")\n",
    "            f.write(\"*\" * 50 + \"\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(f\"Shape of DataFrame: {shape}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(\"Column Information:\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(info + \"\\n\") \n",
    "            f.write(\"\\n\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(\"Missing Values in Each Column:\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(str(null) + \"\\n\")\n",
    "            f.write(\"\\n\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(\"Descriptive Statistics (Numerical):\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(str(des) + \"\\n\")\n",
    "            # f.write(tabulate(des, headers='keys', tablefmt='psql'))\n",
    "            f.write(\"\\n\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(\"First 5 Rows of the DataFrame:\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(str(head) + \"\\n\")\n",
    "            f.write(\"\\n\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(\"Last 5 Rows of the DataFrame:\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(str(tail) + \"\\n\")\n",
    "            f.write(\"\\n\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(f\"Number of Duplicate Rows: {dup}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\\n\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(\"Correlation Matrix (Numerical Columns):\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "            f.write(str(corr_matrix) + \"\\n\")\n",
    "            f.write(\"\\n\\n\")\n",
    "        \n",
    "        print(f\"Report successfully saved as '{report}'.\")\n",
    "     except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def General_Analysis(df):\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERAL ANALYSIS OF THE DATAFRAME\".center(60))\n",
    "    print(\"=\" * 60 + \"\\n\")\n",
    "    \n",
    "    pd.options.display.float_format = '{:,.2f}'.format\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 200)\n",
    "    shape = df.shape\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Shape of DataFrame: {shape}\")\n",
    "    print(\"-\" * 60 + \"\\n\")\n",
    "    print(\"COLUMN INFORMATION\".center(60, \"-\"))\n",
    "    buffer = StringIO()\n",
    "    df.info(buf=buffer)\n",
    "    info = buffer.getvalue()\n",
    "    print(info)\n",
    "    print(\"\\n\")\n",
    "    print(\"MISSING VALUES IN EACH COLUMN\".center(60, \"-\"))\n",
    "    null_values = df.isnull().sum()\n",
    "    print(null_values)\n",
    "    print(\"\\n\")\n",
    "    numdf = df.select_dtypes(include=['number']).dropna()\n",
    "    print(\"DESCRIPTIVE STATISTICS (NUMERICAL)\".center(60, \"-\"))\n",
    "    des = numdf.describe()\n",
    "    print(tabulate(des, headers='keys', tablefmt='psql'))\n",
    "    print(\"\\n\")\n",
    "    print(\"FIRST 5 ROWS OF THE DATAFRAME\".center(60, \"-\"))\n",
    "    head = df.head()\n",
    "    print(tabulate(head, headers='keys', tablefmt='psql'))\n",
    "    print(\"\\n\")\n",
    "    print(\"LAST 5 ROWS OF THE DATAFRAME\".center(60, \"-\"))\n",
    "    tail = df.tail()\n",
    "    print(tabulate(tail, headers='keys', tablefmt='psql'))\n",
    "    print(\"\\n\")\n",
    "    print(\"DUPLICATE ROWS\".center(60, \"-\"))\n",
    "    dup = df.duplicated().sum()\n",
    "    print(f\"Number of Duplicate Rows: {dup}\")\n",
    "    print(\"\\n\")\n",
    "    print(\"CORRELATION MATRIX (NUMERICAL COLUMNS)\".center(60, \"-\"))\n",
    "    corr_matrix = numdf.corr()\n",
    "    print(tabulate(corr_matrix, headers='keys', tablefmt='psql'))\n",
    "    print(\"\\n\")\n",
    "    rows, cols = corr_matrix.shape\n",
    "    max_label_length = max(len(label) for label in corr_matrix.columns)\n",
    "    fig_width = max(8, cols * 0.5, max_label_length * 0.5)\n",
    "    fig_height = max(6, rows * 0.5)\n",
    "    plt.figure(figsize=(fig_width, fig_height))\n",
    "    plt.imshow(corr_matrix, cmap='coolwarm', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.xticks(np.arange(cols), corr_matrix.columns, rotation=45, ha=\"right\", fontsize=12)\n",
    "    plt.yticks(np.arange(rows), corr_matrix.columns, rotation=0, fontsize=12)\n",
    "    plt.title('Correlation Matrix Heatmap', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    save = input('Save General Report? (Y/N): ')\n",
    "    if save.lower()==\"y\":\n",
    "        Save_Genreal_Report(corr_matrix, dup, head, tail, des, null_values, info, shape)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_Analysis(df, col1, col2, num):\n",
    "    col_names = {col.lower(): col for col in df.columns}\n",
    "    col1_actual = col_names.get(col1.lower())\n",
    "    col2_actual = col_names.get(col2.lower())\n",
    "    if not col1_actual or not col2_actual:\n",
    "        print(f\"‚ùå Error: One or both columns '{col1}' or '{col2}' not found in DataFrame.\")\n",
    "        return\n",
    "    \n",
    " \n",
    "    \n",
    "    # df = df[[col1_actual, col2_actual]].copy()\n",
    "\n",
    "    # # Convert columns to numeric (handle errors)\n",
    "    df[col1_actual] = pd.to_numeric(df[col1_actual], errors='coerce')\n",
    "    df[col2_actual] = pd.to_numeric(df[col2_actual], errors='coerce')\n",
    "\n",
    "    # # Handle missing values\n",
    "    df.dropna(inplace=True)  # Remove rows with NaNs\n",
    "    # # OR: Fill missing values with mean\n",
    "    # df[col1_actual].fillna(0, inplace=True)\n",
    "    # df[col2_actual].fillna(0, inplace=True)\n",
    "\n",
    "    # # Remove duplicates\n",
    "    # df.drop_duplicates(inplace=True)\n",
    "    column1_data = df[col1_actual]\n",
    "    column2_data = df[col2_actual]\n",
    "\n",
    "    print(\"=============================== Preview ================================\")\n",
    "    column1_data_df = column1_data.head().to_frame()\n",
    "    column2_data_df = column2_data.head().to_frame()\n",
    "    print(f\"Data for {col1_actual}:\\n\", tabulate(column1_data_df, headers='keys', tablefmt='psql'), \"\\n\")\n",
    "    print(f\"Data for {col2_actual}:\\n\", tabulate(column2_data_df, headers='keys', tablefmt='psql'), \"\\n\")\n",
    "    print(f\"================================ Summary Statistics for {col1_actual} ======================================\")\n",
    "    column1_stats = column1_data.describe()\n",
    "\n",
    "    # Convert the statistics to a DataFrame\n",
    "    column1_stats_df = column1_stats.to_frame().reset_index()\n",
    "    print(tabulate(column1_stats_df, headers=['Statistic', col1_actual], tablefmt='psql'), \"\\n\")\n",
    "    \n",
    "    print(f\"====================================== Summary Statistics for {col2_actual} ========================================\")\n",
    "    column2_stats = column2_data.describe()\n",
    "\n",
    "    # Convert the statistics to a DataFrame\n",
    "    column2_stats_df = column2_stats.to_frame().reset_index()\n",
    "    print(tabulate(column2_stats_df, headers=['Statistic', col2_actual], tablefmt='psql'), \"\\n\")\n",
    "    # print(column2_data.describe(), \"\\n\")\n",
    "    null_count_col1 = column1_data.isnull().sum()\n",
    "    empty_count_col1 = (column1_data.eq('')).sum()\n",
    "    null_count_col2 = column2_data.isnull().sum()\n",
    "    empty_count_col2 = (column2_data.eq('')).sum()\n",
    "    print(f\"Null values in {col1_actual}: {null_count_col1}\")\n",
    "    print(f\"Empty values in {col1_actual}: {empty_count_col1}\\n\")\n",
    "    \n",
    "    print(f\"Null values in {col2_actual}: {null_count_col2}\")\n",
    "    print(f\"Empty values in {col2_actual}: {empty_count_col2}\\n\")\n",
    "    correlation = column1_data.corr(column2_data)\n",
    "    print(f\"Correlation between {col1_actual} and {col2_actual}: {correlation}\\n\")\n",
    "\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "   \n",
    "    sample_df = df.sample(n=1000, random_state=42)  # Reduce to 1000 points\n",
    "   \n",
    "\n",
    "    plt.subplot(2, 2, 1)\n",
    "\n",
    "# Add jitter to spread overlapping points\n",
    "    jitter_x = np.random.normal(0, 0.05, size=len(sample_df[col1_actual]))\n",
    "    jitter_y = np.random.normal(0, 0.05, size=len(sample_df[col2_actual]))\n",
    "\n",
    "    plt.scatter(sample_df[col1_actual] + jitter_x, sample_df[col2_actual] + jitter_y, alpha=0.3)\n",
    "    plt.xlabel(col1_actual)\n",
    "    plt.ylabel(col2_actual)\n",
    "    plt.title(\"Scatter Plot (Jittered)\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "\n",
    "# Smooth the data using rolling mean\n",
    "    rolling_window = 500  # Adjust as needed\n",
    "    df['RollingMean'] = df[col2_actual].rolling(window=rolling_window, min_periods=1).mean()\n",
    "\n",
    "    # Create a clean line plot with markers\n",
    "    plt.plot(df.index, df['RollingMean'], linestyle='-')\n",
    "\n",
    "    plt.xlabel(col1_actual)\n",
    "    plt.ylabel(col2_actual)\n",
    "    plt.title(f\"Line Plot (Smoothed, Window={rolling_window})\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.5)  # Optional: Add grid for readability\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(df[col1_actual], bins=30, alpha=0.7, density=True, color='darkblue', label=col1_actual)  \n",
    "    plt.hist(df[col2_actual], bins=30, alpha=0.7, density=True, color='darkred', label=col2_actual)  \n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.title(\"Histogram (Optimized)\")\n",
    "\n",
    "\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    corr_matrix = df[[col1_actual, col2_actual]].corr()\n",
    "    plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], [col1_actual, col2_actual], rotation=45)\n",
    "    plt.yticks([0, 1], [col1_actual, col2_actual])\n",
    "    plt.title(\"Correlation Heatmap\")\n",
    "\n",
    "    # Adjust layout and show plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # print(1)\n",
    "    # col_names = {col.lower(): col for col in df.columns}\n",
    "    # print(2)\n",
    "    # col1_actual = col_names.get(col1.lower())\n",
    "    # print(3)\n",
    "    # col2_actual = col_names.get(col2.lower())\n",
    "    # print(4)\n",
    "    # if not col1_actual or not col2_actual:\n",
    "    #     print(f\"‚ùå Error: One or both columns '{col1}' or '{col2}' not found in DataFrame.\")\n",
    "    #     return\n",
    "\n",
    "    # # Extracting column data\n",
    "    # df[col1_actual] = pd.to_numeric(df[col1_actual], errors='coerce')\n",
    "    # print(5)\n",
    "\n",
    "    # column1_data = df[col1_actual]\n",
    "    # print(6)\n",
    "    # column2_data = df[col2_actual]\n",
    "    # print(7)\n",
    "\n",
    "    # # ====== Data Cleaning ======\n",
    "    # df = df[[col1_actual, col2_actual]].copy()  # Keep only relevant columns\n",
    "    # print(8)\n",
    "    # df.fillna(0, inplace=True)  # Replace NaN values with 0\n",
    "    # print(9)\n",
    "    # df = df.drop_duplicates()  # Remove duplicate rows\n",
    "    # print(10)\n",
    "    \n",
    "    # # Check if the column contains only non-numeric data\n",
    "    # if pd.to_numeric(df[col1_actual], errors='coerce').isna().all():\n",
    "    #     print(f\"‚ùå Error: Column '{col1_actual}' contains only non-numeric data.\")\n",
    "    #     return\n",
    "    # if pd.to_numeric(df[col2_actual], errors='coerce').isna().all():\n",
    "    #     print(f\"‚ùå Error: Column '{col2_actual}' contains only non-numeric data.\")\n",
    "    #     return\n",
    "   \n",
    "    \n",
    "    # df[col1_actual] = pd.to_numeric(df[col1_actual], errors='coerce')  # Ensure numeric\n",
    "    # print(11)\n",
    "    # df[col2_actual] = pd.to_numeric(df[col2_actual], errors='coerce')\n",
    "    # print(12)\n",
    "    # df.fillna(0, inplace=True)  # Replace NaNs after type conversion with 0\n",
    "    # print(13)\n",
    "    # df = df[(df[col1_actual] >= 0) & (df[col2_actual] >= 0)]  # Remove negative values\n",
    "    # print(7)\n",
    "    \n",
    "    # print(\"=============================== Preview ================================\")\n",
    "    # print(f\"Data for {col1_actual}:\")\n",
    "    # print(tabulate(df[[col1_actual]].head(), headers='keys', tablefmt='psql'), \"\\n\")\n",
    "    # print(f\"Data for {col2_actual}:\")\n",
    "    # print(tabulate(df[[col2_actual]].head(), headers='keys', tablefmt='psql'), \"\\n\")\n",
    "    \n",
    "    # print(f\"================================ Summary Statistics for {col1_actual} ======================================\")\n",
    "    # print(tabulate(df[col1_actual].describe().to_frame().reset_index(), headers=['Statistic', col1_actual], tablefmt='psql'), \"\\n\")\n",
    "    \n",
    "    # print(f\"================================ Summary Statistics for {col2_actual} ======================================\")\n",
    "    # print(tabulate(df[col2_actual].describe().to_frame().reset_index(), headers=['Statistic', col2_actual], tablefmt='psql'), \"\\n\")\n",
    "    \n",
    "    # null_count_col1 = column1_data.isnull().sum()\n",
    "    # null_count_col2 = column2_data.isnull().sum()\n",
    "    # print(f\"Null values in {col1_actual}: {null_count_col1}\")\n",
    "    # print(f\"Null values in {col2_actual}: {null_count_col2}\\n\")\n",
    "    \n",
    "    # correlation = df[col1_actual].corr(df[col2_actual])\n",
    "    # print(f\"Correlation between {col1_actual} and {col2_actual}: {correlation}\\n\")\n",
    "    \n",
    "    # plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # sample_df = df.sample(n=min(1000, len(df)), random_state=42)  # Sample up to 1000 points\n",
    "    \n",
    "    # plt.subplot(2, 2, 1)\n",
    "    # jitter_x = np.random.normal(0, 0.05, size=len(sample_df[col1_actual]))\n",
    "    # jitter_y = np.random.normal(0, 0.05, size=len(sample_df[col2_actual]))\n",
    "    # plt.scatter(sample_df[col1_actual] + jitter_x, sample_df[col2_actual] + jitter_y, alpha=0.3)\n",
    "    # plt.xlabel(col1_actual)\n",
    "    # plt.ylabel(col2_actual)\n",
    "    # plt.title(\"Scatter Plot (Jittered)\")\n",
    "    \n",
    "    # plt.subplot(2, 2, 2)\n",
    "    # rolling_window = 500\n",
    "    # df['RollingMean'] = df[col2_actual].rolling(window=rolling_window, min_periods=1).mean()\n",
    "    # plt.plot(df.index, df['RollingMean'], linestyle='-')\n",
    "    # plt.xlabel(col1_actual)\n",
    "    # plt.ylabel(col2_actual)\n",
    "    # plt.title(f\"Line Plot (Smoothed, Window={rolling_window})\")\n",
    "    # plt.grid(True, linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # plt.subplot(2, 2, 3)\n",
    "    # plt.hist(df[col1_actual], bins=30, alpha=0.7, density=True, color='darkblue', label=col1_actual)  \n",
    "    # plt.hist(df[col2_actual], bins=30, alpha=0.7, density=True, color='darkred', label=col2_actual)  \n",
    "    # plt.legend()\n",
    "    # plt.xlabel(\"Value\")\n",
    "    # plt.ylabel(\"Density\")\n",
    "    # plt.title(\"Histogram (Optimized)\")\n",
    "    \n",
    "    # plt.subplot(2, 2, 4)\n",
    "    # corr_matrix = df[[col1_actual, col2_actual]].corr()\n",
    "    # plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    # plt.colorbar()\n",
    "    # plt.xticks([0, 1], [col1_actual, col2_actual], rotation=45)\n",
    "    # plt.yticks([0, 1], [col1_actual, col2_actual])\n",
    "    # plt.title(\"Correlation Heatmap\")\n",
    "    \n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_string_column(df, column_name, top_n=10):\n",
    "    if column_name not in df.columns:\n",
    "        print(f\"Column '{column_name}' not found in DataFrame.\")\n",
    "        return\n",
    "    \n",
    "    # Count occurrences of each unique value\n",
    "    value_counts = Counter(df[column_name].dropna())  # Drop NaN values if any\n",
    "\n",
    "    # If the number of unique categories exceeds 'top_n', combine the less frequent ones into 'Others'\n",
    "    if len(value_counts) > top_n:\n",
    "        # Get the most common 'top_n' categories\n",
    "        most_common = dict(value_counts.most_common(top_n))\n",
    "        \n",
    "        # Calculate the sum of the remaining categories and assign it to 'Others'\n",
    "        others_count = sum(count for _, count in value_counts.items() if _ not in most_common)\n",
    "        most_common[\"Others\"] = others_count\n",
    "        \n",
    "        value_counts = most_common\n",
    "\n",
    "    # Extract labels and their counts\n",
    "    labels = list(value_counts.keys())\n",
    "    counts = list(value_counts.values())\n",
    "\n",
    "    # Create a figure\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot 1: Vertical Bar Chart\n",
    "    plt.subplot(2, 2, 1)  # 2 rows, 2 columns, 1st plot\n",
    "    plt.bar(labels, counts, color='darkblue')\n",
    "    plt.xlabel(\"Category\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(f\"Distribution of {column_name} (Top {top_n} Categories)\")\n",
    "\n",
    "    # Add category-wise totals (above each bar)\n",
    "    for i, count in enumerate(counts):\n",
    "        plt.text(i, count + 0.1, str(count), ha='center', fontsize=12, fontweight='bold', color='black')\n",
    "\n",
    "    # Plot 2: Pie Chart\n",
    "    plt.subplot(2, 2, 2)  # 2 rows, 2 columns, 2nd plot\n",
    "    plt.pie(counts, labels=labels, autopct='%1.1f%%', colors=plt.cm.Paired.colors)\n",
    "    plt.title(f\"Distribution of {column_name} (Pie Chart)\")\n",
    "\n",
    "    # Plot 3: Word Cloud (Only for text-based columns)\n",
    "    plt.subplot(2, 2, 3)  # 2 rows, 2 columns, 3rd plot\n",
    "    if df[column_name].dtype == 'object':  # Check if column contains textual data\n",
    "        text = \" \".join(df[column_name].dropna())\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(text)\n",
    "\n",
    "        plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Word Cloud for {column_name}\")\n",
    "    else:\n",
    "        plt.axis('off')  # Hide axis if no word cloud\n",
    "    \n",
    "    # Plot 4: Heatmap (Alternative Visualization)\n",
    "    plt.subplot(2, 2, 4)  # 2 rows, 2 columns, 4th plot\n",
    "    # Heatmap: Convert counts into a DataFrame and display it as a heatmap-like effect\n",
    "    heatmap_data = np.array(counts).reshape(1, -1)  # Reshape to a 2D array for heatmap-like effect\n",
    "    plt.imshow(heatmap_data, aspect='auto', cmap='Blues')\n",
    "    plt.colorbar(label=\"Count\")\n",
    "    plt.title(f\"Heatmap for {column_name}\")\n",
    "    plt.xticks(ticks=np.arange(len(labels)), labels=labels, rotation=90)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Basic Analysis\n",
    "    print(f\"\\nBasic Analysis of '{column_name}':\")\n",
    "    print(f\"Total unique values (Top {top_n} with 'Others'): {len(value_counts)}\")\n",
    "    for label, count in value_counts.items():\n",
    "        print(f\"{label}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Analyze_Data(df):\n",
    "    b=True\n",
    "    while(b):\n",
    "        print(\"1. General Analysis\")\n",
    "        print(\"2. Custom Analysis\")\n",
    "        print(\"3. Return\")\n",
    "        choice=input(\"Enter Choice: \").strip()\n",
    "        if(choice==\"1\"):\n",
    "           General_Analysis(df)\n",
    "        elif(choice==\"2\"):\n",
    "            b1=True\n",
    "            while(b1):\n",
    "                print(\"1. Analyze Two Numeric Columns\")\n",
    "                print(\"2. Analyze One Numeric and One String\")\n",
    "                print(\"3. Analyze One Numeric Column\")\n",
    "                print(\"4. Return\")\n",
    "                choice=input(\"Enter Choice: \").strip()\n",
    "                if(choice==\"1\"):\n",
    "                  col1=input('Enter 1st Numeric Column')\n",
    "                  col2=input('Enter 2nd Numeric Column')\n",
    "                  Custom_Analysis(df, col1, col2, 1)\n",
    "                  \n",
    "                elif(choice==\"2\"):\n",
    "                    # col1=input('Enter 1st Numeric Column')\n",
    "                    col2=input('Enter String Column')\n",
    "                    plot_string_column(df, col2)\n",
    "                elif(choice==\"3\"):\n",
    "                  col1=input('Enter 1st Numeric Column')\n",
    "                  Custom_Analysis(df, col1, col2, 3)\n",
    "                elif (choice==\"4\"):\n",
    "                   b1=False\n",
    "                else:\n",
    "                  print(\"Please Enter Valid Choice !\")\n",
    "        elif(choice==\"3\"):\n",
    "          # print(\"Exiting Program...\")\n",
    "          b=False\n",
    "        else:\n",
    "          print(\"Please Enter Valid Choice !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Data():\n",
    "    b=True\n",
    "    while(b):\n",
    "        print(\"1. Analyze CSV File\")\n",
    "        print(\"2. Analyze Parquet File\")\n",
    "        print(\"3. Analyze Excel File\")\n",
    "        print(\"4. Return\")\n",
    "        choice=input(\"Enter Choice: \").strip()\n",
    "        if(choice==\"1\"):\n",
    "          path = input(\"Enter File Path of CSV File: \")\n",
    "          try:\n",
    "              df = pd.read_csv(path, encoding='unicode_escape')\n",
    "              print(\"egmerjngkjerner fjer\")\n",
    "              Analyze_Data(df)\n",
    "          except (FileNotFoundError, Exception) as e:\n",
    "              print(f\"Error: {e}\")\n",
    "            #   print(\"CSV File not found. Try again !\")\n",
    "               \n",
    "        elif(choice==\"2\"):\n",
    "              path = input(\"Enter File Path of Parquet File: \")\n",
    "              try:\n",
    "                  df = pd.read_parquet(path)\n",
    "                  Analyze_Data(df)\n",
    "              except (FileNotFoundError, Exception) as e:\n",
    "                  print(f\"Error: {e}\")\n",
    "                  print(\"Parquet File not found. Try again !\")\n",
    "        elif(choice==\"3\"):\n",
    "              path = input(\"Enter File Path of Excel File: \")\n",
    "              try:\n",
    "                  df = pd.read_excel(path)\n",
    "                  Analyze_Data(df)\n",
    "              except (FileNotFoundError, Exception) as e:\n",
    "                  print(f\"Error: {e}\")\n",
    "                  print(\"Excel File not found. Try again !\")\n",
    "        elif(choice==\"4\"):\n",
    "          # print(\"Exiting Program...\")\n",
    "          b=False\n",
    "        else:\n",
    "          print(\"Please Enter Valid Choice !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Edit_Data():\n",
    "    b = True\n",
    "    while b:\n",
    "        print(\"1. Add a New Row\")\n",
    "        print(\"2. Update an Existing Row\")\n",
    "        print(\"3. Delete a Row\")\n",
    "        print(\"4. Edit Specific Column Data\")\n",
    "        print(\"5. Fill Missing Data\")\n",
    "        print(\"6. Rename Columns\")\n",
    "        print(\"7. Change Data Type of a Column\")\n",
    "        print(\"8. Sort Data\")\n",
    "        print(\"9. Filter Data\")\n",
    "        print(\"10. Remove Duplicates\")\n",
    "        print(\"11. Return\")\n",
    "        \n",
    "        choice = input(\"Enter Choice: \")\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            print(\"Adding a new row...\")\n",
    "            pass\n",
    "        elif choice == \"2\":\n",
    "            print(\"Updating an existing row...\")\n",
    "            pass\n",
    "        elif choice == \"3\":\n",
    "            print(\"Deleting a row...\")\n",
    "            pass\n",
    "        elif choice == \"4\":\n",
    "            print(\"Editing specific column data...\")\n",
    "            pass\n",
    "        elif choice == \"5\":\n",
    "            print(\"Filling missing data...\")\n",
    "            pass\n",
    "        elif choice == \"6\":\n",
    "            print(\"Renaming columns...\")\n",
    "            pass\n",
    "        elif choice == \"7\":\n",
    "            print(\"Changing data type of a column...\")\n",
    "            pass\n",
    "        elif choice == \"8\":\n",
    "            print(\"Sorting data...\")\n",
    "            pass\n",
    "        elif choice == \"9\":\n",
    "            print(\"Filtering data...\")\n",
    "            pass\n",
    "        elif choice == \"10\":\n",
    "            print(\"Removing duplicates...\")\n",
    "            pass\n",
    "        elif choice == \"11\":\n",
    "            print(\"Returning to main menu...\")\n",
    "            b = False\n",
    "        else:\n",
    "            print(\"Please Enter a Valid Choice!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=True\n",
    "while(b):\n",
    "    print(\"1. Analyze Data\")\n",
    "    print(\"2. Edit Data\")\n",
    "    print(\"3. Cleaning Data\")\n",
    "    print(\"4. Sort Folder\")\n",
    "    print(\"5. Exit\")\n",
    "    choice=input(\"Enter Choice: \")\n",
    "    if(choice==\"1\"):\n",
    "        Load_Data()\n",
    "    elif(choice==\"2\"):\n",
    "        print(2)\n",
    "        pass\n",
    "    elif(choice==\"3\"):\n",
    "        print(3)\n",
    "        pass\n",
    "    elif(choice==\"4\"):\n",
    "        print(4)\n",
    "    elif(choice==\"5\"):\n",
    "          print(\"Exiting Program...\")\n",
    "          b=False\n",
    "    else:\n",
    "        print(\"Please Enter Valid Choice !\")\n",
    "# C:\\\\Users\\\\Shivam\\\\Downloads\\\\Diwali Sales Data.csv\n",
    "# C:\\\\Users\\\\Shivam\\\\Downloads\\\\road_accident_dataset.csv\n",
    "# D:\\\\shivam\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
